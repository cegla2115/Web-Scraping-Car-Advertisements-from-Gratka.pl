#dane 

import csv
from urllib import request
from bs4 import BeautifulSoup as BS

base_url = 'https://gratka.pl/motoryzacja/osobowe?page='

# Step 1: Extract links and save to CSV
with open('advertisements.csv', 'w', newline='', encoding='utf-8') as csvfile:
    fieldnames = ['link', 'name']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()

    for page in range(1, 21):
        url = base_url + str(page)
        html = request.urlopen(url)
        bs = BS(html.read(), 'html.parser')

        tags = bs.find_all('a', class_='teaserLink')

        for tag in tags:
            link = tag['href']
            writer.writerow({'link': link})

# Step 2: Extract names and update CSV
with open('advertisements.csv', 'r', newline='', encoding='utf-8') as csvfile:
    reader = csv.DictReader(csvfile)
    rows = list(reader)

with open('advertisements.csv', 'w', newline='', encoding='utf-8') as csvfile:
    fieldnames = ['link', 'name', 'przebieg', 'rok_produkcji', 'rodzaj_paliwa']
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()

    for row in rows:
        link = row['link']
        html = request.urlopen(link)
        bs = BS(html.read(), 'html.parser')

        # name
        name_element = bs.find('h1', class_='sticker__title')
        if name_element:
            name = name_element.get_text(strip=True)
        else:
            print(f"No name element found for link: {link}")
            name = "NaN"
        writer.writerow({'link': link, 'name': name})
        print(name)
